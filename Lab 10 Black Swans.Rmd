---
title: "Lab 10 Black Swans"
author: "Erik Svenneby, Jorge Robelo, Mandy Karako, Ryan Bilkie, Thorbin Atwell"
date: "11/1/2019"
output: html_document
---
```{r,include=FALSE}
#for this to work store the file containing the data on your desktop 
library(dplyr)
library(tidyverse)
library(stringr)
library(knitr)
```
```{r}
Answers<-read_csv("~/Desktop/lab10/Answers_trunc.csv")%>% rename(ScoreA=Score,AnswerDate=CreationDate,X7A=X7,BodyA=Body)
Questions<-read_csv("~/Desktop/lab10/Questions_trunc.csv") %>% rename(ScoreQ=Score,AskedDate=CreationDate,X7Q=X7,BodyQ=Body)
#just makes variables easier to recognize after Join
Q_A<-full_join(Answers,Questions,by="Id")
```

Thorbin's Hypothesis: 

  The number of "if's' and "then's" in a Question should be a proxy of specific and detailed questions which usually net a higher ammount of upvotes. The relationship between these keywords and score should be inverse in the Question domain becuase the amount of ifs and thens indicate a very faceted,complicated answer which regardless of correctness, should net a lower number of upvotes due to people who don't have a solid grasp on the material discussed in the answer. Limitations: answers dont explain if else/then statements in depth lmao, questions asked are asked using logic resembling a flow chart.
```{r}
#Thorbin's code
A<-Answers %>% select(BodyA,AnswerDate,Id,ScoreA) %>% group_by(Id)%>%mutate(keywords=sum(str_detect(BodyA,c("if","If","Iff","iff","then","Then")))) %>% ungroup() %>% group_by(keywords)
Asummarized<-A %>% summarize(observations=n())
# assuming Id is a unique identifier, this gives the amount of ifs and thens in bodyA
Q<-Questions %>% select(BodyQ,AskedDate,Id,ScoreQ) %>% group_by(Id)%>% mutate(keywords=sum(str_detect(BodyQ,c("if","If","Iff","iff","then","Then"))))%>% ungroup() %>% group_by(keywords)
Qsummarized<-Q %>% summarize(observations=n())

## assuming Id is a unique identifier, this gives the amount of ifs and thens in bodyQ
```

#### Analysis:

```{r}
ggplot(data=Q,mapping=aes(x=keywords,y=ScoreQ))+geom_point()+ggtitle("Question upvotes at number of keywords")
kable(Qsummarized,format="html",caption="Summarized answer data at keyword count")
```

  While the Question data appears to directly oppose my hypothesis, it is important to look at the number of questions in each group of keyword counts: 
  
  From this it is clear that the decreasing upvote count is in part due to decreasing number of observations of questions with large numbers of select key words. Since there are still decent numbers of observations up until the 5th category, variability in upvotes appears to decrease with number of key words used, indicating that questions with higher counts keywords tend to be spurned by the stack community. This makes sense because the more If's and then's used likely indicate a question operating at a more advanced level meaning fewer people are going to view it and also meaning that the asker will be intimate with the various complexities of the question.
  
  Turning to the Answers data, at first glance my hypothesis does not appear to be rejected out right. First impression is that there are 2 accepted levels of proxied detail namely from about 0-3 keywords. The summarized data is as follows: 

```{r}
ggplot(data=A,mapping=aes(x=keywords,y=ScoreA))+geom_point()+ggtitle("Answer upvotes at number of keywords")
kable(Asummarized,format="html",caption="Summarized answer data at keyword count")
```

  This table shows that the number of observations across keyword groups changes approximately linearly, which was the case in the previous summarized table. The plot for this data shows that generally variability in percieved answer quality,considering decreasing audience in each percieved level of complexity, tends to increase with the number of key words used. This indicates that for more technical questions requiring the use of more conditional/concluding phrases posess higher likelihood of being highly appreciated answers. To make sense of this it is assumed again that the higher level of complexity questions require more complex answers meaning that the people likley to answer are highly qualified to give said answer, which implies that fewer people will up/down vote the answer because alot of people simply arent at that level of complexity. This is a trend that is observed indicating that this conclusive hypothesis has at least a shred of truth to it. 
  
Conclusion:
  It appears as though my hypothesis is false under the assumed understanding that more complex questions regarding higher level concepts require more complex and faceted answers as measured by the use of conditional language. 

###End Thorbin

---
title: "Lab10"
output: html_document
---

```{r}
answers <- read_csv("C:\\Users\\ryanb\\OneDrive\\Desktop\\Data Science 2019\\Answers_trunc.csv")
questions <- read_csv("C:\\Users\\ryanb\\OneDrive\\Desktop\\Data Science 2019\\Questions_trunc.csv")
colnames(answers)
colnames(questions)
str(answers)
str(questions)
merged <- answers %>%
  left_join(questions, c('ParentId' = 'Id')) 

colnames(merged)
str(merged)

## create relevant columns
merged <- merged %>% 
  mutate(total_time = CreationDate.x - CreationDate.y) %>%
  filter(total_time>0)

```

## Ryan's Hypothesis:
There is an optimal length for questions and an optimal length for answers, and people in general will tend to confine their lengths to that optimal point.

For humans, both time and attention spans are finite. A question or an answer that is too long will be penalized to due either the time-cost or the cognitive load, or both. A question or answer that is too short, on the other hand, risks omitting critical information. Therefore, it will be interesting to use data to help us understand what length of questions or answers tend to result in the highest score.

We begin with the length of a questions' title. Below we see a bar graph showing string length on the x axis and score on the y axis:

```{r}
ggplot(data = questions) +
  geom_bar(mapping = aes(x = str_count(questions$Title), fill = Score))+
  labs(title= "Question Length vs Score", x = "Question Length", y = "Score")
```

While this is not a perfectly normal distribution, it does have a distinct bell-shape, suggesting that question title-lengths closer to the mean tend to result in a higher score.
Indeed, if we calculate the mean, median and standard deviation . . .

```{r}
mean(str_count(questions$Title))
median(str_count(questions$Title))
sd(str_count(questions$Title))
```

. . . we find that title lengths that gravitate more toward the center also happen to be near to where our curve peaks. A standard deviation of 18.99463 is relatively small, suggesting titles have a fairly regular length and questioners tend to restrict their titles to a normal length.

Maybe there is something about the title of a question that leads people to naturally be concise, so we will now turn to the body of the question, where the questioner has more freedom in terms of length. Does it follow the same pattern?

```{r}
ggplot(data = questions) +
  geom_bar(mapping = aes(x = str_count(questions$Body), fill = Score))+
  labs(title= "Question Length vs Score", x = "Question Length", y = "Score")+
  xlim(c(0,5000))
                                                                                   
```

This is messier. Peak scores seem to gravitate somewhere around a question length of 500, but there is a fat tail leading out to the right. However, unlike the case of titles, where the questioners tend to limit their lengths to that optimal point, here we see that this is not the case:

```{r}
mean(str_count(questions$Body))
median(str_count(questions$Body))
sd(str_count(questions$Body))
```

The standard deviation is massive here compared to that of the question titles, and a mean of 826 is further from the optimal point than we would expect if questioners tended to limit their lengths on average to a point that was rewarded with a high score. In other words, although lengthy questions are clearly penalized (since we see from the graph that concise questions get higher scores and longer questions get lower scores), people still tend to err on the side of length.

This makes sense given that concise writing in the ungraduate, graduate, and technical levels is a skill that must be cultivated--it is not the natural tendency of humans. These analyses taken together reveal an interesting tension in human behavior, where the communicator on average imposes a greater time- and attention-cost on the recipient than the recipient deems optimal. And we have just used data to extract a lesson for the communicator.

Finally, we turn to answer length:

```{r}
ggplot(data = answers)+
  geom_bar(mapping = aes(x = str_count(answers$Body), fill = Score))+
  labs(title = "Answer Length vs Score", x ="Answer Length", y = "Score") +
  xlim(c(0, 1000))
```

This is even more interesting: there is a noticable peak in score around a length of 250, and then a very gradual decline as answers lengthen. If we again calculate the statistical properties:

```{r}
mean(str_count(answers$Body))
median(str_count(answers$Body))
sd(str_count(answers$Body))
```

The standard deviation is very large--meaning that answer-lengths vary tremendously--and the mean of (roughly) 658 is very far away from the "optimal point" of 250. (Notice that this is an even greater distance of length than that for questions, which was, roughly, 873-500). This shows that answers tend to err on the side of lengthiness far more than even questions.

What these data show is that a lack of concision is an omnipresent human flaw. It's possible that user feedback over time will (or already has to an extent), teach users to be more concise, but it's also possible that technical-minded people on stackexchange need to understand the art of economical use of language, keeping in mind that humans do not have infinite bandwidth. 

At any rate, the original hypothesis that there is an optimal length has been shown to be true, while the related idea that users will tend to gravitate toward that length has been shown to be true only for question titles and nothing else. Humans grasp that the purpose of a title is to concisely summarize a topic, and this shows that we are able to understand the idea of concision better than we can always implement it--suggesting that it is an acquired skill, not an automatic tendency.


### End Ryan
